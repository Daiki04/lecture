{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python.distribute.distribution_strategy_context'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\daiki\\Documents\\lecture\\lecture6\\main.ipynb セル 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/daiki/Documents/lecture/lecture6/main.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_lecture\\lib\\site-packages\\tensorflow\\__init__.py:45\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m tf2 \u001b[39mas\u001b[39;00m _tf2\n\u001b[0;32m     43\u001b[0m _tf2\u001b[39m.\u001b[39menable()\n\u001b[1;32m---> 45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m __internal__\n\u001b[0;32m     46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m __operators__\n\u001b[0;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m audio\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_lecture\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m dispatch\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m eager_context\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m feature_column\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_lecture\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m interim\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m multi_process_runner\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribution_strategy_context\u001b[39;00m \u001b[39mimport\u001b[39;00m variable_sync_on_read_context\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmerge_call_interim\u001b[39;00m \u001b[39mimport\u001b[39;00m strategy_supports_no_merge_call\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msharded_variable\u001b[39;00m \u001b[39mimport\u001b[39;00m ShardedVariable\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python.distribute.distribution_strategy_context'"
     ]
    }
   ],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daiki\\miniconda3\\envs\\env_lecture\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.data.data_collator because of the following error (look up to see its traceback):\nNo module named 'tensorflow.python.distribute.distribution_strategy_context'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_lecture\\lib\\site-packages\\transformers\\utils\\import_utils.py:1345\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m   1346\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_lecture\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_lecture\\lib\\site-packages\\transformers\\data\\__init__.py:27\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m glue_compute_metrics, xnli_compute_metrics\n\u001b[1;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mprocessors\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     DataProcessor,\n\u001b[0;32m     29\u001b[0m     InputExample,\n\u001b[0;32m     30\u001b[0m     InputFeatures,\n\u001b[0;32m     31\u001b[0m     SingleSentenceClassificationProcessor,\n\u001b[0;32m     32\u001b[0m     SquadExample,\n\u001b[0;32m     33\u001b[0m     SquadFeatures,\n\u001b[0;32m     34\u001b[0m     SquadV1Processor,\n\u001b[0;32m     35\u001b[0m     SquadV2Processor,\n\u001b[0;32m     36\u001b[0m     glue_convert_examples_to_features,\n\u001b[0;32m     37\u001b[0m     glue_output_modes,\n\u001b[0;32m     38\u001b[0m     glue_processors,\n\u001b[0;32m     39\u001b[0m     glue_tasks_num_labels,\n\u001b[0;32m     40\u001b[0m     squad_convert_examples_to_features,\n\u001b[0;32m     41\u001b[0m     xnli_output_modes,\n\u001b[0;32m     42\u001b[0m     xnli_processors,\n\u001b[0;32m     43\u001b[0m     xnli_tasks_num_labels,\n\u001b[0;32m     44\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_lecture\\lib\\site-packages\\transformers\\data\\processors\\__init__.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mglue\u001b[39;00m \u001b[39mimport\u001b[39;00m glue_convert_examples_to_features, glue_output_modes, glue_processors, glue_tasks_num_labels\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msquad\u001b[39;00m \u001b[39mimport\u001b[39;00m SquadExample, SquadFeatures, SquadV1Processor, SquadV2Processor, squad_convert_examples_to_features\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_lecture\\lib\\site-packages\\transformers\\data\\processors\\glue.py:30\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 30\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m     32\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mget_logger(\u001b[39m__name__\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_lecture\\lib\\site-packages\\tensorflow\\__init__.py:45\u001b[0m\n\u001b[0;32m     43\u001b[0m _tf2\u001b[39m.\u001b[39menable()\n\u001b[1;32m---> 45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m __internal__\n\u001b[0;32m     46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m __operators__\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_lecture\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:11\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m dispatch\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m eager_context\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_lecture\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py:11\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m multi_process_runner\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribution_strategy_context\u001b[39;00m \u001b[39mimport\u001b[39;00m variable_sync_on_read_context\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmerge_call_interim\u001b[39;00m \u001b[39mimport\u001b[39;00m strategy_supports_no_merge_call\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python.distribute.distribution_strategy_context'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\daiki\\Documents\\lecture\\lecture6\\main.ipynb セル 1\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/daiki/Documents/lecture/lecture6/main.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m  \u001b[39mneologdn\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/daiki/Documents/lecture/lecture6/main.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/daiki/Documents/lecture/lecture6/main.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m LineByLineTextDataset, DataCollatorForLanguageModeling\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/daiki/Documents/lecture/lecture6/main.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer, TrainingArguments\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_lecture\\lib\\site-packages\\transformers\\utils\\import_utils.py:1335\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[0;32m   1334\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m-> 1335\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[0;32m   1336\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1337\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_lecture\\lib\\site-packages\\transformers\\utils\\import_utils.py:1347\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1345\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m   1346\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 1347\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1348\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1349\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1350\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.data.data_collator because of the following error (look up to see its traceback):\nNo module named 'tensorflow.python.distribute.distribution_strategy_context'"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.util import bigrams\n",
    "import  neologdn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 青空文庫などからデータの取得\n",
    "response = requests.get(\"https://www.aozora.gr.jp/cards/001095/files/42621_21290.html\")\n",
    "response.encoding = response.apparent_encoding\n",
    "\n",
    "# HTML からテキストデータを取得してテキスト部分を取得\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "text = soup.get_text()\n",
    "text = text.split(\"底本：\")[0]\n",
    "text = text.split(\"坂口安吾\\n\\n\\n\\n\\r\\n\\u3000\")[1]\n",
    "text = neologdn.normalize(text)\n",
    "text = re.sub(\"《.+?》\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分かち書き\n",
    "tagger = MeCab.Tagger(\"-Owakati\")\n",
    "words = tagger.parse(text).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram モデルを構築\n",
    "bi_grams = list(bigrams(words))\n",
    "bigram_model = {}\n",
    "for bigram in bi_grams:\n",
    "    if bigram[0] not in bigram_model:\n",
    "        bigram_model[bigram[0]] = [bigram[1]]\n",
    "    else:\n",
    "        bigram_model[bigram[0]].append(bigram[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自分の最も薄い一本あるのはまった。白痴の由であった。天地に落下の色ではようやく終電に似た。\n",
      "自分の姿であった。伊沢自身に一滴の個性だの釜だの首をしてもこの世界を数える時のためにサイパンに\n",
      "自分の置き場所では、伊沢は四月十何本の小屋の穂先すら見えるの消し方が仕掛けて死ぬ、虚妄の破片の\n",
      "自分の豚の空虚で、弾丸もない。ああ、結局娘を表わすから腹と、要するに如何なるアングルによって縛りつけられ、もとよりそれは\n",
      "自分自身に身をねせてあり、女の家では考えた。ひと思いに、人間だの学校の国民酒場を払ってすら\n",
      "自分の無限のあるのふちに法が通勤に旅立った。この煙草に演説を感覚の恥辱であった。違った。街角\n",
      "自分の方が一年昔の場合にすぎないこの女が最後の本質的なものであった、明日と消え失せ、ともかく芸人\n",
      "自分自身が火の出口に、いっしょに関係が、同時に起ったが火も、肺病の賤業中という言外の人はこの差し\n",
      "自分自身の煙草屋が一本ずつビールを歩きかけると何食わぬ顔をむさぼり見た。え?なんと老人の前\n",
      "自分自身のは度の違いそうだった。全てをふかし、それは連日賭場が白んでいる女を逃げだすためであり、私\n"
     ]
    }
   ],
   "source": [
    "# 文章を生成する関数\n",
    "def generate_sentence(model, start_word, max_length):\n",
    "    # 初期単語が設定されていなければランダムに選択\n",
    "    if not start_word:\n",
    "        start_word = random.choice(words)\n",
    "    current_word = start_word\n",
    "    sentence = [current_word]\n",
    "    # max_length だけ繰り返す\n",
    "    for _ in range(max_length):\n",
    "        # 次の単語リストを取得（無ければ終了）\n",
    "        next_words = model.get(current_word)\n",
    "        if not next_words:\n",
    "            break\n",
    "        # 単語リストからランダムに選択して追加\n",
    "        next_word = random.choice(next_words)\n",
    "        sentence.append(next_word)\n",
    "        # 現在の単語を更新\n",
    "        current_word = next_word\n",
    "    return \"\".join(sentence)\n",
    "\n",
    "\n",
    "# ある単語から始まる文章を10個生成\n",
    "for i in range(10):\n",
    "    generated_text = generate_sentence(bigram_model, \"自分\", 30)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# トークナイザーとモデルの準備\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
    "\n",
    "# 青空文庫などからデータの取得\n",
    "response = requests.get(\"https://...\")\n",
    "response.encoding = response.apparent_encoding\n",
    "\n",
    "# HTMLからテキストデータを取得してテキスト部分を取得\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "text = soup.get_text()\n",
    "\n",
    "# 分かち書き\n",
    "tagger = MeCab.Tagger(\"-Owakati\")\n",
    "words = tagger.parse(text)\n",
    "\n",
    "# 前処理したテキストを一旦ファイルに保存\n",
    "with open(\"text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(words)\n",
    "\n",
    "# 訓練用データセットの設定\n",
    "train_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer, file_path=\"text.txt\", block_size=128\n",
    ")\n",
    "\n",
    "# データ入力の設定\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# 訓練の設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./soseki\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    ")\n",
    "\n",
    "# Trainer インスタンスの作成\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# 訓練の実施\n",
    "trainer.train()\n",
    "\n",
    "# 訓練済みモデルの保存\n",
    "trainer.save_model(\"./soseki_model\")\n",
    "\n",
    "# 文章生成のためのコード\n",
    "input_ids = tokenizer.encode(\"吾輩は\", return_tensors=\"pt\")\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,  # 確率的にテキストを生成\n",
    "    max_length=20,  # 生成する文章の最大長\n",
    "    temperature=0.9,  # ランダム性の度合い（大きいほどランダム）\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=10,  # 生成する文章の数\n",
    ")\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}\".format(tokenizer.decode(sample_output, skip_special_tokens=True)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
